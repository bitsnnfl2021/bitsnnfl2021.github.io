Paper name,Paper Link,Dataset Link,Abstract,submitted by (TA),Difficulty,Domain(s),Code,Tasks [optional]
EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/abs/1905.11946v5,http://image-net.org/download,"Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.",Achleshwar,Easy,"Scalable Image Classification, Convolutional Neural Networks",Y,
BAKSA at SemEval-2020 Task 9: Bolstering CNN with Self-Attention forSentiment Analysis of Code Mixed Text,https://arxiv.org/pdf/2007.10819.pdf,https://github.com/keshav22bansal/BAKSA_IITK/tree/master/data,"Sentiment Analysis of code-mixed text has diversified applications in opinion mining rangingfrom tagging user reviews to identifying social or political sentiments of a sub-population. In thispaper, we present an ensemble architecture of convolutional neural net (CNN) and self-attentionbased LSTM for sentiment analysis of code-mixed tweets. While the CNN component helps inthe classification of positive and negative tweets, the self-attention based LSTM, helps in theclassification of neutral tweets, because of its ability to identify correct sentiment among multiplesentiment bearing units. We achieved F1 scores of 0.707 (ranked5th) and 0.725 (ranked13th) onHindi-English (Hinglish) and Spanish-English (Spanglish) datasets, respectively. The submissionsfor Hinglish and Spanglish tasks were made under the usernamesayushkandharsh6respectively.",Shaily,easy," Natural Language Processing, Sentiment Analysis, CNN, Attention",Y,
Deep Residual Learning for Image Recognition,https://arxiv.org/pdf/1512.03385.pdf,https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz,"Deeper neural networks are more difficult to train. We
present a residual learning framework to ease the training
of networks that are substantially deeper than those used
previously. We explicitly reformulate the layers as learn-
ing residual functions with reference to the layer inputs, in-
stead of learning unreferenced functions. We provide com-
prehensive empirical evidence showing that these residual
networks are easier to optimize, and can gain accuracy from
considerably increased depth. On the ImageNet dataset we
evaluate residual nets with a depth of up to 152 layers—8×
deeper than VGG nets [41] but still having lower complex-
ity. An ensemble of these residual nets achieves 3.57% error
on the ImageNet test set. This result won the 1st place on the
ILSVRC 2015 classification task. We also present analysis
on CIFAR-10 with 100 and 1000 layers.
The depth of representations is of central importance
for many visual recognition tasks. Solely due to our ex-
tremely deep representations, we obtain a 28% relative im-
provement on the COCO object detection dataset. Deep
residual nets are foundations of our submissions to ILSVRC
& COCO 2015 competitions1
, where we also won the 1st
places on the tasks of ImageNet detection, ImageNet local-
ization, COCO detection, and COCO segmentation.",Satyam ,Easy,"Object Detection, Computer Vision",y,
"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",https://arxiv.org/pdf/1502.03044.pdf,https://www.kaggle.com/adityajn105/flickr8k,"Inspired by recent work in machine translation
and object detection, we introduce an attention
based model that automatically learns to describe
the content of images. We describe how we
can train this model in a deterministic manner
using standard backpropagation techniques and
stochastically by maximizing a variational lower
bound. We also show through visualization how
the model is able to automatically learn to fix its
gaze on salient objects while generating the cor-
responding words in the output sequence. We
validate the use of attention with state-of-the-
art performance on three benchmark datasets:
Flickr8k, Flickr30k and MS COCO.",Satyam ,Easy,Image captioning,y,
Rethinking the Inception Architecture for Computer Vision,https://arxiv.org/pdf/1512.00567v3.pdf,http://www.image-net.org/download,"Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety oftasks. Since 2014 very deep convolutional networks startedto become mainstream, yielding substantial gains in vari-ous benchmarks. Although increased model size and com-putational cost tend to translate to immediate quality gainsfor most tasks (as long as enough labeled data is providedfor training), computational efficiency and low parametercount are still enabling factors for various use cases such asmobile vision and big-data scenarios. Here we are explor-ing ways to scale up networks in ways that aim at utilizingthe added computation as efficiently as possible by suitablyfactorized convolutions and aggressive regularization. Webenchmark our methods on the ILSVRC 2012 classificationchallenge validation set demonstrate substantial gains overthe state of the art:21.2%top-1and5.6%top-5error forsingle frame evaluation using a network with a computa-tional cost of5billion multiply-adds per inference and withusing less than 25 million parameters. With an ensemble of4models and multi-crop evaluation, we report3.5%top-5error and17.3%top-1error.",Manasvi,Easy,"Optimization, Convolutional Neural Networks",Y,
Sign Language Recognition Using Deep Learning on Custom Processed Static Gesture Images,https://ieeexplore.ieee.org/document/8537248,https://drive.google.com/file/d/1frZLLkr_AXPGQBw_7takmN-0PM3Na1-A/view?usp=sharing,"Sign Language detection by technology is an overlooked concept despite there being a large social group which could benefit by it. There are not many technologies which help in connecting this social group to the rest of the world. Understanding sign language is one of the primary enablers in helping users of sign language communicate with the rest of the society. Image classification and machine learning can be used to help computers recognize sign language, which could then be interpreted by other people. Convolutional neural networks have been employed in this paper to recognize sign language gestures. The image dataset used consists of static sign language gestures captured on an RGB camera. Preprocessing was performed on the images, which then served as the cleaned input. The paper presents results obtained by retraining and testing this sign language gestures dataset on a convolutional neural network model using Inception v3. The model consists of multiple convolution filter inputs that are processed on the same input. The validation accuracy obtained was above 90% This paper also reviews the various attempts that have been made at sign language detection using machine learning and depth data of images. It takes stock of the various challenges posed in tackling such a problem, and outlines future scope as well.",Manasvi,Easy,"Computer Vision, Object recogntion",N,
Squeeze and Excitation Network,https://arxiv.org/abs/1709.01507,http://image-net.org/ ,"The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to
construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad
range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of
a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel
relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates
channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be
stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate
that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost.
Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and
reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ∼25%",Sanchit,Easy,Compter Vision,Y,
Unsupervised representation learning by predicting image rotations,https://arxiv.org/pdf/1803.07728.pdf,"http://image-net.org, http://host.robots.ox.ac.uk/pascal/VOC/","Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification",Siddharth,Easy,"Unsupervised Learning, Representation Learning",Y,
EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks,https://arxiv.org/pdf/1905.11946.pdf,http://image-net.org,"Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.3% top-1 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.",Siddharth,Easy,"Meta Learning, CNNs",Y,
Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks,https://arxiv.org/pdf/1908.10084.pdf,"https://nlp.stanford.edu/projects/snli/, https://cims.nyu.edu/~sbowman/multinli/","BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering.
In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT.
We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. ",Harsh,Easy,"Natural Language Processing, Transformers",Y,
Weight Uncertainty in Neural Networks,https://arxiv.org/pdf/1505.05424v2.pdf,http://yann.lecun.com/exdb/mnist/,"We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.",Kushank,Easy,"Reinforcement Learning, Backpropagation, Bayesian learning",Y,
MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications,https://arxiv.org/pdf/1704.04861v1.pdf,https://cocodataset.org/#download,"We present a class of efficient models called MobileNetsfor mobile and embedded vision applications.  MobileNetsare  based  on  a  streamlined  architecture  that  uses  depth-wise  separable  convolutions  to  build  light  weight  deepneural  networks.   We  introduce  two  simple  global  hyper-parameters  that  efficiently  trade  off  between  latency  andaccuracy. These hyper-parameters allow the model builderto choose the right sized model for their application basedon  the  constraints  of  the  problem.   We  present  extensiveexperiments on resource and accuracy tradeoffs and showstrong performance compared to other popular models onImageNet classification. We then demonstrate the effective-ness of MobileNets across a wide range of applications anduse  cases  including  object  detection,  finegrain  classifica-tion, face attributes and large scale geo-localization",Naitik,Easy,"Object Detection, Computer Vision",Y,
SpinalNet: Deep Neural Network with Gradual Input ,https://arxiv.org/pdf/2007.03347v2.pdf,https://www.cs.toronto.edu/~kriz/cifar.html,"Over the past few years, deep neural networks (DNNs) have garnered remarkable success in a diverse range of real-world applications. However, DNNs consider a large number of inputs and consist of a large number of parameters, resulting in high computational demand. We study the human somatosensory system and propose the SpinalNet to achieve higher accuracy with less computational resources. In a typical neural network (NN) architecture, the hidden layers receive inputs in the first layer and then transfer the intermediate outcomes to the next layer. In the proposed SpinalNet, the structure of hidden layers allocates to three sectors: 1) Input row, 2) Intermediate row, and 3) output row. The intermediate row of the SpinalNet contains a few neurons. The role of input segmentation is in enabling each hidden layer to receive a part of the inputs and outputs of the previous layer. Therefore, the number of incoming weights in a hidden layer is significantly lower than traditional DNNs. As all layers of the SpinalNet directly contributes to the output row, the vanishing gradient problem does not exist. We also investigate the SpinalNet fully-connected layer to several well-known DNN models and perform traditional learning and transfer learning. We observe significant error reductions with lower computational costs in most of the DNNs. We have also obtained the state-of-the-art (SOTA) performance for QMNIST, Kuzushiji-MNIST, EMNIST (Letters, Digits, and Balanced), STL-10, Bird225, Fruits 360, and Caltech-101 datasets.",Naitik,Easy,"Image Classification, Computer Vision",Y,
U-Net: Convolutional Networks for Biomedical Image Segmentation ,https://arxiv.org/pdf/1505.04597v1.pdf,http://brainiac2.mit.edu/isbi_challenge/home,"There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at this http URL . ",Naitik,Easy,"Segmentation, Computer Vision",Y,
Meta Bi-LSTM'(Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings),https://arxiv.org/pdf/1805.08237v1.pdf,https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2184,"The rise of neural networks, and particularly recurrent neural networks, has produced significant advances in part-of-speech tagging accuracy. One characteristic common among these models is the presence of rich initial word encodings. These encodings typically are composed of a recurrent character-based representation with dynamically and pre-trained word embeddings. However, these encodings do not consider a context wider than a single word and it is only through subsequent recurrent layers that word or sub-word information interacts. In this paper, we investigate models that use recurrent neural networks with sentence-level context for initial character and word-based representations. In particular we show that optimal results are obtained by integrating these context sensitive representations through synchronized training with a meta-model that learns to combine their states.",Manthan,Easy,"Natural Language Processing, POS Tagging",Y,
Playing Atari with Six Neurons,https://arxiv.org/abs/1806.01363v2,https://gym.openai.com/,"Deep reinforcement learning, applied to vision-based problems like Atari games, maps pixels directly to actions; internally, the deep neural network bears the responsibility of both extracting useful information and making decisions based on it. By separating the image processing from decision-making, one could better understand the complexity of each task, as well as potentially find smaller policy representations that are easier for humans to understand and may generalize better. To this end, we propose a new method for learning policies and compact state representations separately but simultaneously for policy approximation in reinforcement learning. State representations are generated by an encoder based on two novel algorithms: Increasing Dictionary Vector Quantization makes the encoder capable of growing its dictionary size over time, to address new observations as they appear in an open-ended online-learning context; Direct Residuals Sparse Coding encodes observations by disregarding reconstruction error minimization, and aiming instead for highest information inclusion. The encoder autonomously selects observations online to train on, in order to maximize code sparsity. As the dictionary size increases, the encoder produces increasingly larger inputs for the neural network: this is addressed by a variation of the Exponential Natural Evolution Strategies algorithm which adapts its probability distribution dimensionality along the run. We test our system on a selection of Atari games using tiny neural networks of only 6 to 18 neurons (depending on the game's controls). These are still capable of achieving results comparable---and occasionally superior---to state-of-the-art techniques which use two orders of magnitude more neurons.",Manan,Hard,"Reinforcement Learning, Neuro evolution",N,
FairMOT: On the Fairness of Detection and Re-Identification in Multiple Object Tracking,https://arxiv.org/pdf/2004.01888v5.pdf,https://motchallenge.net/data/MOT20/,"There has been remarkable progress on object detection and re-identification (re-ID) in recent years which are the key components of multi-object tracking. However, little attention has been focused on jointly accomplishing the two tasks in a single network. Our study shows that the previous attempts ended up with degraded accuracy mainly because the re-ID task is not fairly learned which causes many identity switches. The unfairness lies in two-fold: (1) they treat re-ID as a secondary task whose accuracy heavily depends on the primary detection task. So training is largely biased to the detection task but ignores the re-ID task; (2) they use ROI-Align to extract re-ID features which is directly borrowed from object detection. However, this introduces a lot of ambiguity in characterizing objects because many sampling points may belong to disturbing instances or background. To solve the problems, we present a simple approach FairMOT which consists of two homogeneous branches to predict pixel-wise objectness scores and re-ID features. The achieved fairness between the tasks allows FairMOT to obtain high levels of detection and tracking accuracy and outperform previous state-of-the-arts by a large margin on several public datasets",Achleshwar,Hard,"Object Tracking, Computer Vision",Y,
End-to-End Object Detection with Transformers.,https://arxiv.org/pdf/2005.12872v3.pdf,https://cocodataset.org/#download,"We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines.",Tanuj,Hard,"Object Detection, Computer Vision",Y,
Improving the Transformer Translation Modelwith Document-Level Context,https://arxiv.org/pdf/1810.03581.pdf,https://github.com/THUNLP-MT/Document-Transformer,"Although the Transformer translation model (Vaswani et al., 2017) has achieved state-of-the-art performance in a variety of translation tasks, how to use document-level con-text to deal with discourse phenomena prob-lematic for Transformer still remains a chal-lenge. In this work, we extend the Transformermodel with a new context encoder to repre-sent document-level context, which is then in-corporated into the original encoder and de-coder. As large-scale document-level paral-lel corpora are usually not available, we intro-duce a two-step training method to take fulladvantage of abundant sentence-level parallelcorpora and limited document-level parallelcorpora. Experiments on the NIST Chinese-English datasets and the IWSLT French-English datasets show that our approach im-proves over Transformer significantly",Shaily,hard,"Natural Language Processing, Transformers, Machine Translation",y,
Stereo R-CNN based 3D Object Detection for Autonomous Driving,https://arxiv.org/pdf/1902.09738.pdf,http://www.cvlibs.net/datasets/kitti.,"We present a novel dataset captured from a VW
station wagon for use in mobile robotics and autonomous driving
research. In total, we recorded 6 hours of traffic scenarios at
10-100 Hz using a variety of sensor modalities such as high-
resolution color and grayscale stereo cameras, a Velodyne 3D
laser scanner and a high-precision GPS/IMU inertial navigation
system. The scenarios are diverse, capturing real-world traffic
situations and range from freeways over rural areas to inner-
city scenes with many static and dynamic objects. Our data is
calibrated, synchronized and timestamped, and we provide the
rectified and raw image sequences. Our dataset also contains
object labels in the form of 3D tracklets and we provide online
benchmarks for stereo, optical flow, object detection and other
tasks. This paper describes our recording platform, the data
format and the utilities that we provide.",Satyam ,Hard,"Object Detection, Computer Vision",y,
"A Deeper Look into Sarcastic Tweets
Using Deep Convolutional Neural Networks",https://sentic.net/sarcasm-detection-with-deep-convolutional-neural-networks.pdf,"Sentiment Dataset: https://www.kaggle.com/iarunava/imdb-movie-reviews-dataset (IMDB Movie Reviews)

Personality Dataset: https://drive.google.com/file/d/1xTg5iJZzzNEf3jJJKhBpgwkMnYDHaKQJ/view?usp=sharing (OCEAN)

Test Dataset: https://github.com/ef2020/SarcasmAmazonReviewsCorpus (Amazon Review Dataset)","Sarcasm detection is a key task for many natural language processing tasks. In sentiment analysis, for example, sarcasm can flip the polarity of an “apparently positive” sentence and, hence, negatively affect polarity detection performance. To date, most approaches to sarcasm detection have treated the task primarily as a text categorization problem. Sarcasm, however, can be expressed in very subtle ways and requires a deeper understanding of natural language that standard text categorization techniques cannot grasp. In this work, we develop models based on a pre-trained convolutional neural network for extracting sentiment, emotion and personality features for sarcasm detection. Such features, along with the network’s baseline features, allow the proposed models to outperform the state of the art on benchmark datasets. We also address the often ignored generalizability issue of classifying data that have not been seen by the models at the learning phase

",Sanchit,Hard,"Natural Language Processing, Classification",Y,
SlowFast Networks for video recognition,https://arxiv.org/pdf/1812.03982.pdf,"https://deepmind.com/research/open-source/kinetics , https://research.google.com/ava/download.html , https://prior.allenai.org/projects/charades","We present SlowFast networks for video recognition. Our model involves (i) a Slow pathway, operating at low frame rate, to capture spatial semantics, and (ii) a Fast pathway, operating at high frame rate, to capture motion at fine temporal resolution. The Fast pathway can be made very lightweight by reducing its channel capacity, yet can learn useful temporal information for video recognition. Our models achieve strong performance for both action classification and detection in video, and large improvements are pin-pointed as contributions by our SlowFast concept. We report state-of-the-art accuracy on major video recognition benchmarks, Kinetics, Charades and AVA. ",Siddharth ,Hard,"Video recognition, ResNet, 3D Conv",Y,
Scale-recurrent Network for Deep Image Deblurring,https://arxiv.org/abs/1802.01770,https://drive.google.com/file/d/1H0PIXvJH4c40pk7ou6nAwoxuR4Qh_Sa2/view,"In single image deblurring, the “coarse-to-fine” scheme, i.e. gradually restoring the sharp image on different resolutions in a pyramid, is very successful in both traditional optimization-based methods and recent neural-network-based approaches. In this paper, we investigate this strategy and propose a Scale-recurrent Network (SRN-DeblurNet) for this deblurring task. Compared with the many recent learning-based approaches in [25], it has a simpler network structure, a smaller number of parameters and is easier to train. We evaluate our method on large-scale deblurring datasets with complex motion. Results show that our method can produce better quality results than state-of-the-arts, both quantitatively and qualitatively.",Kushank,Hard,"Computer Vision, Image De-Blurring, Recurrent Neural Networks",Y,
NATURAL TTS SYNTHESIS BY CONDITIONING WAVENET ON MEL SPECTROGRAM PREDICTIONS(Tacotron2),https://arxiv.org/pdf/1712.05884.pdf,https://keithito.com/LJ-Speech-Dataset/,"This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and F0 features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.",Manthan,Hard,"Natural Language Processing, Text To Speech",Y,
TACOTRON: TOWARDS END-TO-END SPEECH SYNTHESIS,https://arxiv.org/pdf/1703.10135.pdf,https://keithito.com/LJ-Speech-Dataset/,"A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given <text, audio> pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.",Manthan,Hard,"Natural Language Processing, Text To Speech",Y,
Playing Atari with Deep Reinforcement Learning,https://arxiv.org/abs/1312.5602,https://github.com/mgbellemare/Arcade-Learning-Environment,"We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.",Manan,Medium,"Reinforcement Learning, Deep Learning",Y,
PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models,https://arxiv.org/abs/2003.03808,https://www.kaggle.com/lamsimon/celebahq,"The primary aim of single-image super-resolution is to construct high-resolution (HR) images from corresponding low-resolution (LR) inputs. In previous approaches, which have generally been supervised, the training objective typically measures a pixel-wise average distance between the super-resolved (SR) and HR images. Optimizing such metrics often leads to blurring, especially in high variance (detailed) regions. We propose an alternative formulation of the super-resolution problem based on creating realistic SR images that downscale correctly. We present an algorithm addressing this problem, PULSE (Photo Upsampling via Latent Space Exploration), which generates high-resolution, realistic images at resolutions previously unseen in the literature. It accomplishes this in an entirely self-supervised fashion and is not confined to a specific degradation operator used during training, unlike previous methods (which require supervised training on databases of LR-HR image pairs). Instead of starting with the LR image and slowly adding detail, PULSE traverses the high-resolution natural image manifold, searching for images that downscale to the original LR image. This is formalized through the ""downscaling loss,"" which guides exploration through the latent space of a generative model. By leveraging properties of high-dimensional Gaussians, we restrict the search space to guarantee realistic outputs. PULSE thereby generates super-resolved images that both are realistic and downscale correctly. We show proof of concept of our approach in the domain of face super-resolution (i.e., face hallucination). We also present a discussion of the limitations and biases of the method as currently implemented with an accompanying model card with relevant metrics. Our method outperforms state-of-the-art methods in perceptual quality at higher resolutions and scale factors than previously possible.",Manan,Medium,"Computer Vision, Image processing, Generative Adversarial Networks",Y,
Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,https://arxiv.org/abs/1609.04802,https://github.com/jbhuang0604/SelfExSR,"Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.",Manan,Medium,"Image processing, Generative Adversarial Networks, Computer Vision",Y,
Encoding in Style: a StyleGAN Encoder for Image-to-Image Translation,https://arxiv.org/abs/2008.00951v1,"https://github.com/NVlabs/ffhq-dataset, https://www.kaggle.com/lamsimon/celebahq","Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.",Manan,Medium,"Image processing, Generative Adversarial Networks, Computer Vision",Y,
Temporal Segment Networks,https://arxiv.org/abs/1705.02953,https://asankagp.github.io/mod20/,"Deep convolutional networks have achieved great success for image recognition. However, for action recognition in videos, their advantage over traditional methods is not so evident. We present a general and flexible video-level framework for learning action models in videos. This method, called temporal segment network (TSN), aims to model long-range temporal structures with a new segment-based sampling and aggregation module. This unique design enables our TSN to efficiently learn action models by using the whole action videos. The learned models could be easily adapted for action recognition in both trimmed and untrimmed videos with simple average pooling and multi-scale temporal window integration, respectively. We also study a series of good practices for the instantiation of TSN framework given limited training samples. Our approach obtains the state-the-of-art performance on four challenging action recognition benchmarks: HMDB51 (71.0%), UCF101 (94.9%), THUMOS14 (80.1%), and ActivityNet v1.2 (89.6%). Using the proposed RGB difference for motion models, our method can still achieve competitive accuracy on UCF101 (91.0%) while running at 340 FPS. Furthermore, based on the temporal segment networks, we won the video classification track at the ActivityNet challenge 2016 among 24 teams, which demonstrates the effectiveness of TSN and the proposed good practices.",Achleshwar,Medium,"Action Recognition, Computer Vision",Y,
Focal Loss for Dense Object Detection,https://arxiv.org/pdf/1708.02002v2.pdf,https://cocodataset.org/#download,"The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.",Achleshwar,Medium,"Object Detection, Computer Vision",Y,
One-Shot Object Detection with Co-Attention and Co-Excitation,https://arxiv.org/pdf/1911.12529v1.pdf,https://cocodataset.org/#download,"This paper aims to tackle the challenging problem of one-shot object detection. Given a query image patch whose class label is not included in the training data, the goal of the task is to detect all instances of the same class in a target image. To this end, we develop a novel co-attention and co-excitation (CoAE) framework that makes contributions in three key technical aspects. First, we propose to use the nonlocal operation to explore the co-attention embodied in each query-target pair and yield region proposals accounting for the one-shot situation. Second, we formulate a squeeze-and-co-excitation scheme that can adaptively emphasize correlated feature channels to help uncover relevant proposals and eventually the target objects. Third, we design a margin-based ranking loss for implicitly learning a metric to predict the similarity of a region proposal to the underlying query, no matter its class label is seen or unseen in training. The resulting model is therefore a two-stage detector that yields a strong baseline on both VOC and MS-COCO under one-shot setting of detecting objects from both seen and never-seen classes.",Achleshwar,Medium,"Object Detection, One-shot Learning, Computer Vision",Y,
DensePose: Dense Human Pose Estimation In The Wild,https://arxiv.org/pdf/1802.00434v1.pdf,https://cocodataset.org/#densepose-2020,"In this work, we establish dense correspondences between an RGB image and a surface-based representation of the human body, a task we refer to as dense human pose estimation. We first gather dense correspondences for 50K persons appearing in the COCO dataset by introducing an efficient annotation pipeline. We then use our dataset to train CNN-based systems that deliver dense correspondence ‘in the wild’, namely in the presence of background, occlusions and scale variations. We improve our training set’s effectiveness by training an ‘inpainting’ network that can fill in missing ground truth values, and report clear improvements with respect to the best results that would be achievable in the past. We experiment with fullyconvolutional networks and region-based models and observe a superiority of the latter; we further improve accuracy through cascading, obtaining a system that delivers highly-accurate results in real time.",Achleshwar,Medium,"Pose Estimation, Computer Vision",Y,
Sparse-to-Dense: Depth Prediction from Sparse Depth Samples and a Single Image,https://arxiv.org/abs/1709.07492,https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,"We consider the problem of dense depth prediction from a sparse set of depth measurements and a single RGB image. Since depth estimation from monocular images alone is inherently ambiguous and unreliable, to attain a higher level of robustness and accuracy, we introduce additional sparse depth samples, which are either acquired with a low-resolution depth sensor or computed via visual Simultaneous Localization and Mapping (SLAM) algorithms. We propose the use of a single deep regression network to learn directly from the RGB-D raw data, and explore the impact of number of depth samples on prediction accuracy. Our experiments show that, compared to using only RGB images, the addition of 100 spatially random depth samples reduces the prediction root-mean-square error by 50% on the NYU-Depth-v2 indoor dataset. It also boosts the percentage of reliable prediction from 59% to 92% on the KITTI dataset. We demonstrate two applications of the proposed algorithm: a plug-in module in SLAM to convert sparse maps to dense maps, and super-resolution for LiDARs",Tanuj,Medium,"Depth Estimation, Computer Vision ",Y,
Dual Attention Network for Scene Segmentation,https://arxiv.org/pdf/1809.02983v4.pdf,https://www.cityscapes-dataset.com/downloads/,"In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the selfattention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5% on Cityscapes test set is achieved without using coarse data.",Tanuj,Medium,"Segmantation, Computer Vision",Y,
One-Shot Instance Segmentation,https://arxiv.org/pdf/1811.11507v2.pdf,https://cocodataset.org/#download,"We tackle the problem of one-shot instance segmentation: Given an example image of a novel, previously unknown object category, find and segment all objects of this category within a complex scene. To address this challenging new task, we propose Siamese Mask R-CNN. It extends Mask R-CNN by a Siamese backbone encoding both reference image and scene, allowing it to target detection and segmentation towards the reference category. We demonstrate empirical results on MS Coco highlighting challenges of the one-shot setting: while transferring knowledge about instance segmentation to novel object categories works very well, targeting the detection network towards the reference category appears to be more difficult. Our work provides a first strong baseline for one-shot instance segmentation and will hopefully inspire further research into more powerful and flexible scene analysis algorithms.",Tanuj,Medium,"One-Short Learning, Segmentation, Computer Vision",Y,
"Delete, Retrieve, Generate:A Simple Approach to Sentiment and Style Transfer",https://arxiv.org/pdf/1804.06437.pdf,https://github.com/rpryzant/delete_retrieve_generate/tree/master/data,"We consider the task of text attribute transfer:transforming a sentence to alter a specific at-tribute (e.g., sentiment) while preserving itsattribute-independent content (e.g., changing“screen is just the right size”to“screen is toosmall”). Our training data includes only sen-tences labeled with their attribute (e.g., pos-itive or negative), but not pairs of sentencesthat differ only in their attributes, so we mustlearn to disentangle attributes from attribute-independent content in an unsupervised way.Previous work using adversarial methods hasstruggled to produce high-quality outputs. Inthis paper, we propose simpler methods mo-tivated by the observation that text attributesare often marked by distinctive phrases (e.g.,“too small”). Our strongest method extractscontent words by deleting phrases associatedwith the sentence’s original attribute value, re-trieves new phrases associated with the targetattribute, and uses a neural model to fluentlycombine these into a final output. On humanevaluation, our best method generates gram-matical and appropriate responses on22%more inputs than the best previous system, av-eraged over three attribute transfer datasets:altering sentiment of reviews on Yelp, alteringsentiment of reviews on Amazon, and alteringimage captions to be more romantic or humor-ous",Shaily,medium,"Natural Language Processing, Style Transfer",Y,
Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification,https://arxiv.org/pdf/1908.11860.pdf,https://www.yelp.com/dataset/download ; https://www.yelp.com/dataset/download ,"Aspect-TargetSentimentClassification(ATSC) is a subtask of Aspect-Based Sen-timent Analysis (ABSA), which has manyapplications e.g. in e-commerce, where dataand insights from reviews can be leveragedto create value for businesses and customers.Recently,deep transfer-learning methodshave been applied successfully to a myriadof Natural Language Processing (NLP) tasks,including ATSC. Building on top of the promi-nent BERT language model, we approachATSC using a two-step procedure:self-supervised domain-specific BERT languagemodel finetuning, followed by supervisedtask-specific finetuning.Our findings onhow to best exploit domain-specific languagemodel finetuning enable us to produce newstate-of-the-art performance on the SemEval2014 Task 4 restaurants dataset. In addition,to explore the real-world robustness of ourmodels, we perform cross-domain evaluation.We show that a cross-domain adapted BERTlanguage model performs significantly betterthan strong baseline models like vanillaBERT-base and XLNet-base.Finally, weconduct a case study to interpret modelprediction errors",Shaily,medium,"Natural Language Processing, Sentiment Analysis",Y,
"Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",https://arxiv.org/pdf/1808.08745.pdf,https://github.com/EdinburghNLP/XSum/tree/master/XSum-Dataset,"We introduce extreme summarization, a new single-document summarization task whichdoes not favor extractive strategies and callsfor an abstractive modeling approach. The idea is to create a short, one-sentence newssummary answering the question “What is thearticle about?”. We collect a real-world, largescale dataset for this task by harvesting onlinearticles from the British Broadcasting Corpo-ration (BBC). We propose a novel abstrac-tive model which is conditioned on the ar-ticle’s topics and based entirely on convolu-tional neural networks. We demonstrate exper-imentally that this architecture captures long-range dependencies in a document and recog-nizes pertinent content, outperforming an or-acle extractive system and state-of-the-art ab-stractive approaches when evaluated automat-ically and by humans",Shaily,medium,"Natural Language Processing, Summarization",y,
Higher-order Coreference Resolution with Coarse-to-fine Inference,https://arxiv.org/pdf/1804.05392.pdf,https://github.com/kentonl/e2e-coref,"We introduce a fully differentiable approxima-tion to higher-order inference for coreferenceresolution. Our approach uses the antecedentdistribution from a span-ranking architectureas an attention mechanism to iteratively re-fine span representations. This enables themodel to softly consider multiple hops in thepredicted clusters. To alleviate the computa-tional cost of this iterative process, we intro-duce a coarse-to-fine approach that incorpo-rates a less accurate but more efficient bilin-ear factor, enabling more aggressive pruningwithout hurting accuracy. Compared to the ex-isting state-of-the-art span-ranking approach,our model significantly improves accuracy onthe English OntoNotes benchmark, while be-ing far more computationally efficient",Shaily,medium,"Natural Language Processing, Coreference Resoultion",y,
CE-Net: Context Encoder Network for 2D Medical Image Segmentation,https://arxiv.org/pdf/1903.02740.pdf,https://drive.google.com/open?id=1VPCvVsPgrfPNIl932xgU3XC_WFLUsXJR,"Abstract—Medical image segmentation is an important step
in medical image analysis. With the rapid development of
convolutional neural network in image processing, deep learning
has been used for medical image segmentation, such as optic
disc segmentation, blood vessel detection, lung segmentation, cell
segmentation, etc. Previously, U-net based approaches have been
proposed. However, the consecutive pooling and strided convolu-
tional operations lead to the loss of some spatial information. In
this paper, we propose a context encoder network (referred to as
CE-Net) to capture more high-level information and preserve
spatial information for 2D medical image segmentation. CE-
Net mainly contains three major components: a feature encoder
module, a context extractor and a feature decoder module. We
use pretrained ResNet block as the fixed feature extractor. The
context extractor module is formed by a newly proposed dense
atrous convolution (DAC) block and residual multi-kernel pooling
(RMP) block. We applied the proposed CE-Net to different 2D
medical image segmentation tasks. Comprehensive results show
that the proposed method outperforms the original U-Net method
and other state-of-the-art methods for optic disc segmentation,
vessel detection, lung segmentation, cell contour segmentation
and retinal optical coherence tomography layer segmentation.",Satyam ,Medium,"Segmentation, Object Detection, Computer Vision",y,
Gradient Centralization: A New Optimization Technique for Deep Neural Networks,https://arxiv.org/pdf/2004.01461v2.pdf,https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz,"Optimization techniques are of great importance to effectively and efficiently train a deep neural network (DNN). It has been shown that using the first and second order statistics (e.g., mean and variance) to perform Z-score standardization on network activations or weight vectors, such as batch normalization (BN) and weight standardization (WS), can improve the training performance. Different from these existing methods that mostly operate on activations or weights, we present a newoptimization technique, namely gradient centralization (GC), which operates directly on gradients by centralizing the gradient vectors to have zero mean. GC can be viewed as a projected gradient descent method with a constrained loss function. We show that GC can regularize both the weight space and output feature space so that it can boost the generalization performance of DNNs. Moreover, GC improves the Lipschitzness of the loss function and its gradient so that the training process becomes more efficient and stable. GC is very simple to implement and can be easily embedded into existing gradient based DNN optimizers with only one line of code. It can also be directly used to finetune the pre-trained DNNs. Our experiments on various applications, including general image classification, fine-grained image classification, detection and segmentation, demonstrate that GC can consistently improve the performance of DNN learning. The code of GC can be found at https://github.com/Yonghongwei/Gradient-Centralization.",Satyam ,Medium,"Optimization, Deep Learning",y,
Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,https://arxiv.org/pdf/1511.06434.pdf,https://www.kaggle.com/dataset/504743cb487a5aed565ce14238c6343b7d650ffd28c071f03f2fd9b25819e6c9,"In recent years, supervised learning with convolutional networks (CNNs) has
seen huge adoption in computer vision applications. Comparatively, unsupervised
learning with CNNs has received less attention. In this work we hope to help
bridge the gap between the success of CNNs for supervised learning and unsuper-
vised learning. We introduce a class of CNNs called deep convolutional generative
adversarial networks (DCGANs), that have certain architectural constraints, and
demonstrate that they are a strong candidate for unsupervised learning. Training
on various image datasets, we show convincing evidence that our deep convolu-
tional adversarial pair learns a hierarchy of representations from object parts to
scenes in both the generator and discriminator. Additionally, we use the learned
features for novel tasks - demonstrating their applicability as general image repre-
sentations.",Satyam ,Medium,"Generative Adversarial Networks, Image generation",y,
EfficientDet: Scalable and Efficient Object Detection,https://arxiv.org/pdf/1911.09070.pdf,https://cocodataset.org/#download,"Model efficiency has become increasingly important incomputer vision. In this paper, we systematically study neu-ral network architecture design choices for object detectionand propose several key optimizations to improve efficiency.First, we propose a weighted bi-directional feature pyra-mid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scal-ing method that uniformly scales the resolution, depth, andwidth for all backbone, feature network, and box/class pre-diction networks at the same time. Based on these optimiza-tions and better backbones, we have developed a new familyof object detectors, called EfficientDet, which consistentlyachieve much better efficiency than prior art across a widespectrum of resource constraints. In particular, with single-model and single-scale, our EfficientDet-D7 achieves state-of-the-art55.1 APon COCOtest-devwith 77M param-eters and 410B FLOPs1, being4x – 9xsmaller and using13x – 42xfewer FLOPs than previous detectors.",Manasvi,Medium,"Object Detection, Computer Vision",Y,
"You Only Look Once:Unified, Real-Time Object Detection",https://homes.cs.washington.edu/~ali/papers/YOLO.pdf,http://www.image-net.org/download,"We present YOLO, a new approach to object detection.Prior work on object detection repurposes classifiers to per-form detection. Instead, we frame object detection as a re-gression problem to spatially separated bounding boxes andassociated class probabilities. A single neural network pre-dicts bounding boxes and class probabilities directly fromfull images in one evaluation. Since the whole detectionpipeline is a single network, it can be optimized end-to-enddirectly on detection performance.Our unified architecture is extremely fast. Our baseYOLO model processes images in real-time at 45 framesper second. A smaller version of the network, Fast YOLO,processes an astounding 155 frames per second whilestill achieving double the mAP of other real-time detec-tors. Compared to state-of-the-art detection systems, YOLOmakes more localization errors but is less likely to predictfalse positives on background. Finally, YOLO learns verygeneral representations of objects. It outperforms other de-tection methods, including DPM and R-CNN, when gener-alizing from natural images to other domains like artwork",Manasvi,Medium,"Object Detection, Computer Vision",Y,
Optic-Net: A Novel Convolutional Neural Networkfor Diagnosis of Retinal Diseases from OpticalTomography Images,https://arxiv.org/pdf/1910.05672v1.pdf,https://www.kaggle.com/paultimothymooney/kermany2018,"Abstract—Diagnosing different retinal diseases from SpectralDomain Optical Coherence Tomography (SD-OCT) images is achallenging task. Different automated approaches such as imageprocessing, machine learning and deep learning algorithms havebeen used for early detection and diagnosis of retinal diseases.Unfortunately, these are prone to error and computationalinefficiency, which requires further intervention from humanexperts. In this paper, we propose a novel convolution neuralnetwork architecture to successfully distinguish between differentdegeneration of retinal layers and their underlying causes.The proposed novel architecture outperforms other classificationmodels while addressing the issue of gradient explosion. Ourapproach reaches near perfect accuracy of 99.8% and 100% fortwo separately available Retinal SD-OCT data-set respectively.Additionally, our architecture predicts retinal diseases in realtime while outperforming human diagnosticians.Keywords—SD-OCT, Convolutional Neural Networks, RetinalDegeneration; Residual Neural Network; Deep Learning; Com-puter Vision",Manasvi,Medium,"Segmentation, Computer Vision",Y,
DoubleU-Net: A Deep Convolutional NeuralNetwork for Medical Image Segmentation,https://arxiv.org/pdf/2006.04868v2.pdf,https://polyp.grand-challenge.org/databases/,"Semantic image segmentation is the process of la-beling each pixel of an image with its corresponding class. Anencoder-decoder based approach, like U-Net and its variants, isa popular strategy for solving medical image segmentation tasks.To improve the performance of U-Net on various segmentationtasks, we propose a novel architecture called DoubleU-Net, whichis a combination of two U-Net architectures stacked on top of eachother. The first U-Net uses a pre-trained VGG-19 as the encoder,which has already learned features from ImageNet and can betransferred to another task easily. To capture more semanticinformation efficiently, we added another U-Net at the bottom.We also adopt Atrous Spatial Pyramid Pooling (ASPP) to capturecontextual information within the network. We have evaluatedDoubleU-Net using four medical segmentation datasets, coveringvarious imaging modalities such as colonoscopy, dermoscopy, andmicroscopy. Experiments on the 2015 MICCAI sub-challenge onautomatic polyp detection dataset, the CVC-ClinicDB, the 2018Data Science Bowl challenge, and the Lesion boundary segmen-tation datasets demonstrate that the DoubleU-Net outperformsU-Net and the baseline models. Moreover, DoubleU-Net producesmore accurate segmentation masks, especially in the case of theCVC-ClinicDB and 2015 MICCAI sub-challenge on automaticpolyp detection dataset, which have challenging images such assmaller and flat polyps. These results show the improvementover the existing U-Net model. The encouraging results, pro-duced on various medical image segmentation datasets, showthat DoubleU-Net can be used as a strong baseline for bothmedical image segmentation and cross-dataset evaluation testingto measure the generalizability of Deep Learning (DL) models.Index Terms—semantic segmentation, convolutional neuralnetwork, U-Net, DoubleU-Net, CVC-ClinicDB, ETIS-Larib,ASPP, 2015 MICCAI sub-challenge on automatic polyp detec-tion, 2018 Data Science Bowl, Lesion Boundary Segmentationchallenge",Manasvi,Medium,"Segmentation, Computer Vision",Y,
Style Transfer Through Back-Translation,https://arxiv.org/pdf/1804.09000.pdf,"Political slant transfer - http://tts.speech.cs.cmu.edu/style_models/political_data.tar
Gender Transfer - http://tts.speech.cs.cmu.edu/style_models/gender_data.tar
Multiple attribute text rewriting - http://tts.speech.cs.cmu.edu/style_models/yelp_reviews.txt
http://tts.speech.cs.cmu.edu/style_models/yelp_attrs.txt","Style transfer is the task of rephrasing the
text to contain specific stylistic properties without changing the intent or affect
within the context. This paper introduces
a new method for automatic style transfer. We first learn a latent representation of
the input sentence which is grounded in a
language translation model in order to better preserve the meaning of the sentence
while reducing stylistic properties. Then
adversarial generation techniques are used
to make the output match the desired style.
We evaluate this technique on three different style transformations: sentiment,
gender and political slant. Compared
to two state-of-the-art style transfer modeling techniques we show improvements
both in automatic evaluation of style transfer and in manual evaluation of meaning
preservation and fluency",Sanchit,Medium,"Natural Language Processing, Style Transfer",Y,
Neural Machine Translation by Jointly Learning to Align and Translate,https://arxiv.org/abs/1409.0473,"Dataset: Train (English, Vietnamese)
Dataset: Test (English, Vietnamese)","Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encoder source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts like a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",Sanchit,Medium,"Natural Language Processing, Machine Translation, Attention",Y,
Deep Pyramid Convolutional Neural Networks for Text Categorization,https://pdfs.semanticscholar.org/2f79/66bd3bc7aaf64c7db40fb7f3309f5207cbf7.pdf?_ga=2.81462615.1581587070.1564247907-742172260.1564247907,Yelp full review dataset -https://s3.amazonaws.com/fast-ai-nlp/yelp_review_full_csv.tgz,"This paper proposes a low-complexity word-level deep convolutional neural network (CNN) architecture for text categorization that can efficiently represent long-range associations in text. In the literature, several deep and complex neural networks have been proposed for this task, assuming availability of relatively large amounts of training data. However, the associated computational complexity increases as the networks go deeper, which poses serious challenges in practical applications. Moreover, it was shown recently that shallow word-level CNNs are more accurate and much faster than the state-of-the-art very deep nets such as character-level CNNs even in the setting of large training data. Motivated by these findings, we carefully studied deepening of word-level CNNs to capture global representations of text, and found a simple network architecture with which the best accuracy can be obtained by increasing the network depth without increasing computational cost by much. We call it deep pyramid CNN. The proposed model with 15 weight layers outperforms the previous best models on six benchmark datasets for sentiment classification and topic categorization.",Sanchit,Medium,"Natural Language Processing, Classification",Y,
Attention Is All You Need,https://arxiv.org/abs/1706.03762,http://www.cs.toronto.edu/~pekhimenko/tbd/datasets.html,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",Siddharth,Medium,"Natural Language Processing, Transformers",Y,
TransGAN: Two Transformers Can Make One Strong GAN,https://arxiv.org/abs/2102.07074v2,"https://cs.stanford.edu/~acoates/stl10/ , https://www.cs.toronto.edu/~kriz/cifar.html","The recent explosive interest on transformers has suggested their potential to become powerful ""universal"" models for computer vision tasks, such as classification, detection, and segmentation. However, how further transformers can go - are they ready to take some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs)? Driven by that curiosity, we conduct the first pilot study in building a GAN \textbf{completely free of convolutions}, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed \textbf{TransGAN}, consists of a memory-friendly transformer-based generator that progressively increases feature resolution while decreasing embedding dimension, and a patch-level discriminator that is also transformer-based. We then demonstrate TransGAN to notably benefit from data augmentations (more than standard GANs), a multi-task co-training strategy for the generator, and a locally initialized self-attention that emphasizes the neighborhood smoothness of natural images. Equipped with those findings, TransGAN can effectively scale up with bigger models and high-resolution image datasets. Specifically, our best architecture achieves highly competitive performance compared to current state-of-the-art GANs based on convolutional backbones. Specifically, TransGAN sets \textbf{new state-of-the-art} IS score of 10.10 and FID score of 25.32 on STL-10. It also reaches competitive 8.64 IS score and 11.89 FID score on Cifar-10, and 12.23 FID score on CelebA 64×64, respectively. We also conclude with a discussion of the current limitations and future potential of TransGAN.",Siddharth,Medium,"Transformers, GAN , Image Generation",Y,
DeepCluster Unsupervised learning of visual features,https://arxiv.org/abs/1807.05520,"http://image-net.org , http://projects.dfki.uni-kl.de/yfcc100m/","Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.",Siddharth,Medium,"Unsupervised Learning, Clustering",Y,
Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts,https://arxiv.org/pdf/1906.01267.pdf,https://github.com/NUSTM/ECPE/tree/master/data_combine,"Emotion cause extraction (ECE), the task aimed at extracting the potential causes behind certain emotions in text, has gained much attention in recent years due to its wide applications. However, it suffers from two shortcomings: 1) the emotion must be annotated before cause extraction in ECE, which greatly limits its applications in real-world scenarios; 2) the way to first annotate emotion and then extract the cause ignores the fact that they are mutually indicative. In this work, we propose a new task: emotion-cause pair extraction (ECPE), which aims to extract the potential pairs of emotions and corresponding causes in a document. We propose a 2-step approach to address this new ECPE task, which first performs individual emotion extraction and cause extraction via multi-task learning, and then conduct emotion-cause pairing and filtering. The experimental results on a benchmark emotion cause corpus prove the feasibility of the ECPE task as well as the effectiveness of our approach. ",Harsh,Medium,"Natural Language Processing, Attention",Y,
 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer ,https://arxiv.org/abs/1910.10683v3,https://www.tensorflow.org/datasets/catalog/c4,"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more",Harsh,Medium,"Natural Language Processing, Transfer Learning",Y,
Text Summarization with Pretrained Encoders,https://arxiv.org/abs/1908.08345v2,"https://www.tensorflow.org/datasets/catalog/cnn_dailymail, https://paperswithcode.com/dataset/new-york-times-annotated-corpus, https://www.tensorflow.org/datasets/catalog/xsumm","Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings",Harsh,Medium,"Natural Language Processing, Summarization",Y,
DeblurGAN: Blind Motion Deblurring Using Conditional Adversarial Networks,https://arxiv.org/pdf/1711.07064.pdf,https://drive.google.com/file/d/1H0PIXvJH4c40pk7ou6nAwoxuR4Qh_Sa2/view,"We present DeblurGAN, an end-to-end learned method for motion deblurring. The learning is based on a conditional GAN and the content loss . DeblurGAN achieves state-of-the art performance both in the structural similarity measure and visual appearance. The quality of the deblurring model is also evaluated in a novel way on a real-world problem – object detection on (de-)blurred images. The method is 5 times faster than the closest competitor – DeepDeblur. We also introduce a novel method for generating synthetic motion blurred images from sharp ones, allowing realistic dataset augmentation.",Kushank,Medium,"GAN, Computer Vision, Image De-Blurring",Y,
Multi-Level Factorization Net for person Re-Identification,https://arxiv.org/pdf/1803.09132.pdf,https://www.kaggle.com/pengcw1/market-1501/data,"Key to effective person re-identification (Re-ID) is modelling discriminative and view-invariant factors of person appearance at both high and low semantic levels. Recently developed deep Re-ID models either learn a holistic single semantic level feature representation and/or require laborious human annotation of these factors as attributes. We propose Multi-Level Factorisation Net (MLFN), a novel network architecture that factorises the visual appearance of a person into latent discriminative factors at multiple semantic levels without manual annotation. MLFN is composed of multiple stacked blocks. Each block contains multiple factor modules to model latent factors at a specific level, and factor selection modules that dynamically select the factor modules to interpret the content of each input image. The outputs of the factor selection modules also provide a compact latent factor descriptor that is complementary to the conventional deeply learned features. MLFN achieves state-of-the-art results on three Re-ID datasets, as well as compelling results on the general object categorisation CIFAR-100 dataset.",Kushank,Medium,"Person Re-Identification, Convolutional Neural Networks, Deep Learning",N,
Siamese Neural Networks for One-shot Image Recognition,https://www.cs.cmu.edu//~rsalakhu/papers/oneshot1.pdf,https://github.com/brendenlake/omniglot,"The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks.",Kushank,Medium,"Siamese Network, One-Shot Recgnition",Y,
 ShakeDrop Regularization for Deep Residual Learning ,https://arxiv.org/pdf/1802.02375v3.pdf,https://www.cs.toronto.edu/~kriz/cifar.html,"Overfitting is a crucial problem in deep neural networks, even in the latest network architectures. In this paper, to relieve the overfitting effect of ResNet and its improvements (i.e., Wide ResNet, PyramidNet, and ResNeXt), we propose a new regularization method called ShakeDrop regularization. ShakeDrop is inspired by Shake-Shake, which is an effective regularization method, but can be applied to ResNeXt only. ShakeDrop is more effective than Shake-Shake and can be applied not only to ResNeXt but also ResNet, Wide ResNet, and PyramidNet. An important key is to achieve stability of training. Because effective regularization often causes unstable training, we introduce a training stabilizer, which is an unusual use of an existing regularizer. Through experiments under various conditions, we demonstrate the conditions under which ShakeDrop works well.",Naitik,Medium,"Optimization , Convolutional Neural Networks , Computer Vision",Y,
Training Generative Adversarial Networks with Limited Data ,https://arxiv.org/pdf/2006.06676v2.pdf,https://www.cs.toronto.edu/~kriz/cifar.html,"Training generative adversarial networks (GAN) using too little data typically leads to discriminator overfitting, causing training to diverge. We propose an adaptive discriminator augmentation mechanism that significantly stabilizes training in limited data regimes. The approach does not require changes to loss functions or network architectures, and is applicable both when training from scratch and when fine-tuning an existing GAN on another dataset. We demonstrate, on several datasets, that good results are now possible using only a few thousand training images, often matching StyleGAN2 results with an order of magnitude fewer images. We expect this to open up new application domains for GANs. We also find that the widely used CIFAR-10 is, in fact, a limited data benchmark, and improve the record FID from 5.59 to 2.42.",Naitik,Medium,"Image Generation, Computer Vision, Generative Adversarial Networks",Y,
Wavenet,https://arxiv.org/pdf/1609.03499v2.pdf,https://keithito.com/LJ-Speech-Dataset/,"This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.",Manthan,Medium,"Natural Language Processing, Text To Speech",Y,
XLNet: Generalized Autoregressive Pretraining for Language Understanding,https://arxiv.org/pdf/1906.08237.pdf,open entity,"With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.",Manthan,Medium,"Natural Language Processing, Named Entity Recognition",Y,
Evaluating the Utility of Hand-crafted Features in Sequence Labelling∗,https://arxiv.org/pdf/1808.09075v1.pdf,https://github.com/synalp/NER/tree/master/corpus/CoNLL-2003,"Conventional wisdom is that hand-crafted features are redundant for deep learning models, as they already learn adequate representations of text automatically from corpora. In this work, we test this claim by proposing a new method for exploiting handcrafted features as part of a novel hybrid learning approach, incorporating a feature auto-encoder loss component. We evaluate on the task of named entity recognition (NER), where we show that including manual features for part-of-speech, word shapes and gazetteers can improve the performance of a neural CRF model. We obtain a F 1 of 91.89 for the CoNLL-2003 English shared task, which significantly outperforms a collection of highly competitive baseline models. We also present an ablation study showing the importance of auto-encoding, over using features as either inputs or outputs alone, and moreover, show including the autoencoder components reduces training requirements to 60%, while retaining the same predictive accuracy.",Manthan,Medium,"Natural Language Processing, Named Entity Recognition",Y,
"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",https://arxiv.org/abs/1705.07750,https://serre-lab.clps.brown.edu/resource/breakfast-actions-dataset/,"The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics.",Manthan,Medium,"Natural Language Processing, Named Entity Recognition",Y,
,,,,,,,,
AI Programmer: Autonomously Creating Software Programs Using Genetic Algorithms,https://arxiv.org/abs/1709.05703v1,-,"In this paper, we present the first-of-its-kind machine learning (ML) system, called AI Programmer, that can automatically generate full software programs requiring only minimal human guidance. At its core, AI Programmer uses genetic algorithms (GA) coupled with a tightly constrained programming language that minimizes the overhead of its ML search space. Part of AI Programmer's novelty stems from (i) its unique system design, including an embedded, hand-crafted interpreter for efficiency and security and (ii) its augmentation of GAs to include instruction-gene randomization bindings and programming language-specific genome construction and elimination techniques. We provide a detailed examination of AI Programmer's system design, several examples detailing how the system works, and experimental data demonstrating its software generation capabilities and performance using only mainstream CPUs.",Manan,Easy,Genetic Algorithms,,
A PID Controller Approach for Stochastic Optimization of Deep Networks,https://openaccess.thecvf.com/content_cvpr_2018/papers/An_A_PID_Controller_CVPR_2018_paper.pdf,https://www.cs.toronto.edu/~kriz/cifar.html,"Deep neural networks have demonstrated their power in many computer vision applications. State-of-the-art deep architectures such as VGG, ResNet, and DenseNet are mostly optimized by the SGD-Momentum algorithm, which updates the weights by considering their past and current gradients. Nonetheless, SGD-Momentum suffers from the overshoot problem, which hinders the convergence of network training. Inspired by the prominent success of proportional-integral-derivative (PID) controller in automatic control, we propose a PID approach for accelerating deep network optimization. We first reveal the intrinsic connections between SGD-Momentum and PID based controller, then present the optimization algorithm which exploits the past, current, and change of gradients to update the network parameters. The proposed PID method reduces much the overshoot phenomena of SGD-Momentum, and it achieves up to 50% acceleration on popular deep network architectures with competitive accuracy, as verified by our experiments on the benchmark datasets including CIFAR10, CIFAR100, and Tiny-ImageNet.",Achleshwar,Medium,"Optimization, Computer Vision, PID",Y,
 On the Variance of the Adaptive Learning Rate and Beyond ,https://arxiv.org/abs/1908.03265v3,"https://www.cs.toronto.edu/~kriz/cifar.html, http://image-net.org/download","The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method",Naitik,Medium,"Optimization, Computer Vision",Y,