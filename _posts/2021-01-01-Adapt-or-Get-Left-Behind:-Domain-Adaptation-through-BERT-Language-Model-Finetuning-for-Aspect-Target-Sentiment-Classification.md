---
layout: post
author:
  name: Paper ID 18
  difficulty: medium
share: true
title: Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification
categories:
- Natural Language Processing
- Sentiment Analysis
- medium

tags: []

---
**Abstract** - Aspect-TargetSentimentClassification(ATSC) is a subtask of Aspect-Based Sen-timent Analysis (ABSA), which has manyapplications e.g. in e-commerce, where dataand insights from reviews can be leveragedto create value for businesses and customers.Recently,deep transfer-learning methodshave been applied successfully to a myriadof Natural Language Processing (NLP) tasks,including ATSC. Building on top of the promi-nent BERT language model, we approachATSC using a two-step procedure:self-supervised domain-specific BERT languagemodel finetuning, followed by supervisedtask-specific finetuning.Our findings onhow to best exploit domain-specific languagemodel finetuning enable us to produce newstate-of-the-art performance on the SemEval2014 Task 4 restaurants dataset. In addition,to explore the real-world robustness of ourmodels, we perform cross-domain evaluation.We show that a cross-domain adapted BERTlanguage model performs significantly betterthan strong baseline models like vanillaBERT-base and XLNet-base.Finally, weconduct a case study to interpret modelprediction errors

**Paper** - [https://arxiv.org/pdf/1908.11860.pdf](https://arxiv.org/pdf/1908.11860.pdf)

**Dataset -** [https://www.yelp.com/dataset/download ; https://www.yelp.com/dataset/download ](https://www.yelp.com/dataset/download ; https://www.yelp.com/dataset/download )
    